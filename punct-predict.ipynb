{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['全て###0',\n",
       " 'の###0',\n",
       " '現実###0',\n",
       " 'を###0',\n",
       " '自分###0',\n",
       " 'の###0',\n",
       " '方###0',\n",
       " 'に###0',\n",
       " '捻じ###0',\n",
       " '曲げる###0',\n",
       " '性格###0',\n",
       " 'に###0',\n",
       " '育っ###0',\n",
       " 'て###0',\n",
       " 'しまっ###0',\n",
       " 'た###0',\n",
       " '誠に###0',\n",
       " '遺憾###0',\n",
       " 'で###0',\n",
       " 'ある###0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "import torch\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "from postagger_elmo import PosDatasetReader, LstmTagger\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "token_indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "reader = PosDatasetReader(token_indexers={\"tokens\": token_indexer})\n",
    "\n",
    "HIDDEN_DIM = 100\n",
    "weight_file = 'weights.hdf5'  # 'https://elmoja.blob.core.windows.net/elmoweights/weights.hdf5'\n",
    "options_file = 'options.json'  # 'https://elmoja.blob.core.windows.net/elmoweights/options.json'\n",
    "\n",
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "\n",
    "lstm: Seq2SeqEncoder = PytorchSeq2SeqWrapper(\n",
    "        torch.nn.LSTM(word_embeddings.get_output_dim(), HIDDEN_DIM, bidirectional=True, batch_first=True))\n",
    "\n",
    "# And here's how to reload the model.\n",
    "vocab2 = Vocabulary.from_files(\"postagger-elmo_result/vocabulary\")\n",
    "model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
    "with open(\"postagger-elmo_result/model.th\", 'rb') as f:\n",
    "    model2.load_state_dict(torch.load(f))\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "else:\n",
    "    cuda_device = -1\n",
    "if cuda_device > -1:\n",
    "    model2.cuda(cuda_device)\n",
    "\n",
    "predictor2 = SentenceTaggerPredictor(model2, dataset_reader=reader)\n",
    "sent = \"全て の 現実 を 自分 の 方 に 捻じ 曲げる 性格 に 育っ て しまっ た 誠に 遺憾 で ある\"\n",
    "tag_logits2 = predictor2.predict(sent)['tag_logits']\n",
    "\n",
    "assert(len(sent.split(' ')) == len(tag_logits2))\n",
    "[f'{token}###{str(np.argmax(logit))}' for token, logit in zip(sent.split(' '), tag_logits2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['全て###[0.86934222 0.07292952 0.05772826]',\n",
       " 'の###[0.8887794  0.05801573 0.05320488]',\n",
       " '現実###[0.92986737 0.03687602 0.03325661]',\n",
       " 'を###[0.93516603 0.03664256 0.02819141]',\n",
       " '自分###[0.9263598  0.03904786 0.03459234]',\n",
       " 'の###[0.91478141 0.04648098 0.0387376 ]',\n",
       " '方###[0.89343189 0.06799893 0.03856917]',\n",
       " 'に###[0.9203937  0.04862519 0.03098111]',\n",
       " '捻じ###[0.90269846 0.06443285 0.03286869]',\n",
       " '曲げる###[0.88541345 0.06538274 0.0492038 ]',\n",
       " '性格###[0.91606951 0.05133766 0.03259283]',\n",
       " 'に###[0.89106047 0.06614343 0.0427961 ]',\n",
       " '育っ###[0.91435369 0.05150008 0.03414623]',\n",
       " 'て###[0.89343151 0.06506117 0.04150732]',\n",
       " 'しまっ###[0.93601506 0.036106   0.02787894]',\n",
       " 'た###[0.85053127 0.08446402 0.06500471]',\n",
       " '誠に###[0.89127032 0.0598754  0.04885428]',\n",
       " '遺憾###[0.84652487 0.08621997 0.06725517]',\n",
       " 'で###[0.83627946 0.09924991 0.06447063]',\n",
       " 'ある###[0.88468529 0.06223108 0.05308363]']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{token}###{str(softmax(logit))}' for token, logit in zip(sent.split(' '), tag_logits2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'全て###1 の###2 現実###0 を###0 自分###0 の###0 方###1 に###0 捻じ###1 曲げる###1 性格###0 に###1 育っ###0 て###1 しまっ###0 た###1 誠に###0 遺憾###1 で###1 ある###2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_punct(prob, touten_th=0.06, kuten_th=0.05):\n",
    "    touten_p, kuten_p = prob[1], prob[2]\n",
    "    touten = touten_p > touten_th\n",
    "    kuten = kuten_p > kuten_th\n",
    "    if touten and kuten:\n",
    "        return 1 if touten_p - 0.01 > kuten_p else 2\n",
    "    elif touten and not kuten:\n",
    "        return 1\n",
    "    elif not touten and kuten:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "' '.join([f'{token}###{to_punct(softmax(logit))}' for token, logit in zip(sent.split(' '), tag_logits2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['全て###0',\n",
       " 'の###0',\n",
       " '現実###0',\n",
       " 'を###0',\n",
       " '自分###0',\n",
       " 'の###0',\n",
       " '方###0',\n",
       " 'に###0',\n",
       " '捻じ###0',\n",
       " '曲げる###0',\n",
       " '性格###0',\n",
       " 'に###0',\n",
       " '育っ###0',\n",
       " 'て###0',\n",
       " 'しまっ###0',\n",
       " 'た###0',\n",
       " '誠に###0',\n",
       " '遺憾###0',\n",
       " 'で###0',\n",
       " 'ある###0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "import torch\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "from postagger_elmo import PosDatasetReader, LstmTagger\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "token_indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "reader = PosDatasetReader()\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "# And here's how to reload the model.\n",
    "vocab2 = Vocabulary.from_files(\"postagger-bilstm_result10k/vocabulary\")\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab2.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM,\n",
    "                            pretrained_file=\"wikipedia_mecab_word2vec/word2vec_glovefmt.txt\")\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "lstm: Seq2SeqEncoder = PytorchSeq2SeqWrapper(\n",
    "        torch.nn.LSTM(word_embeddings.get_output_dim(), HIDDEN_DIM, bidirectional=True, batch_first=True))\n",
    "\n",
    "\n",
    "model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
    "with open(\"postagger-bilstm_result10k/model.th\", 'rb') as f:\n",
    "    model2.load_state_dict(torch.load(f))\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "else:\n",
    "    cuda_device = -1\n",
    "if cuda_device > -1:\n",
    "    model2.cuda(cuda_device)\n",
    "predictor2 = SentenceTaggerPredictor(model2, dataset_reader=reader)\n",
    "\n",
    "sent = \"全て の 現実 を 自分 の 方 に 捻じ 曲げる 性格 に 育っ て しまっ た 誠に 遺憾 で ある\"\n",
    "tag_logits2 = predictor2.predict(sent)['tag_logits']\n",
    "\n",
    "assert(len(sent.split(' ')) == len(tag_logits2))\n",
    "[f'{token}###{str(np.argmax(logit))}' for token, logit in zip(sent.split(' '), tag_logits2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['全て###[0.849698   0.08201079 0.06829121]',\n",
       " 'の###[0.87388894 0.06848654 0.05762452]',\n",
       " '現実###[0.88512345 0.06222602 0.05265054]',\n",
       " 'を###[0.89231312 0.0582539  0.04943298]',\n",
       " '自分###[0.896236  0.0560623 0.0477017]',\n",
       " 'の###[0.89930979 0.05437592 0.04631429]',\n",
       " '方###[0.89976951 0.05413136 0.04609912]',\n",
       " 'に###[0.90053401 0.05371032 0.04575567]',\n",
       " '捻じ###[0.90067513 0.05358766 0.04573721]',\n",
       " '曲げる###[0.90062654 0.05360678 0.04576668]',\n",
       " '性格###[0.90044531 0.05371284 0.04584185]',\n",
       " 'に###[0.90062746 0.05359266 0.04577988]',\n",
       " '育っ###[0.89970542 0.0540344  0.04626019]',\n",
       " 'て###[0.8988044  0.05450626 0.04668934]',\n",
       " 'しまっ###[0.89733564 0.05515796 0.0475064 ]',\n",
       " 'た###[0.89482086 0.05633137 0.04884778]',\n",
       " '誠に###[0.89066792 0.05836048 0.0509716 ]',\n",
       " '遺憾###[0.88415709 0.06152737 0.05431555]',\n",
       " 'で###[0.87273862 0.06702554 0.06023584]',\n",
       " 'ある###[0.85159883 0.07714889 0.07125228]']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{token}###{str(softmax(logit))}' for token, logit in zip(sent.split(' '), tag_logits2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'全て###1 の###1 現実###1 を###0 自分###0 の###0 方###0 に###0 捻じ###0 曲げる###0 性格###0 に###0 育っ###0 て###0 しまっ###0 た###0 誠に###0 遺憾###1 で###2 ある###2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_punct(prob, touten_th=0.06, kuten_th=0.055):\n",
    "    touten_p, kuten_p = prob[1], prob[2]\n",
    "    touten = touten_p > touten_th\n",
    "    kuten = kuten_p > kuten_th\n",
    "    if touten and kuten:\n",
    "        return 1 if touten_p - 0.01 > kuten_p else 2\n",
    "    elif touten and not kuten:\n",
    "        return 1\n",
    "    elif not touten and kuten:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "' '.join([f'{token}###{to_punct(softmax(logit))}' for token, logit in zip(sent.split(' '), tag_logits2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
